{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf95654",
   "metadata": {},
   "source": [
    "# build Sagemaker pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4320dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bab23",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e865aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<sagemaker.session.Session at 0x7eff71374b80>,\n",
       " 'arn:aws:iam::527657206104:role/service-role/AmazonSageMaker-ExecutionRole-20230329T093409')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import sagemaker, boto3, json \n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role =  'arn:aws:iam::527657206104:role/service-role/AmazonSageMaker-ExecutionRole-20230329T093409' #sagemaker.get_execution_role()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "sess, aws_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d2f13",
   "metadata": {},
   "source": [
    "## Creating processing step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bb419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Usning PipelineSession \n",
    "SageMaker Pipelines tries to find a previous run of your current pipeline step with the same values for certain attributes. \n",
    "If found, SageMaker Pipelines propagates the outputs from the previous run rather than recomputing the step\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html\n",
    "'''\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tensorflow.processing import TensorFlowProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec58693",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_manifest_file_path = 's3://computer-vision-bootcamp/new_dataset/supermarket-dataset/manifests/output/output.manifest'\n",
    "s3_images_path = 's3://computer-vision-bootcamp/new_dataset/'\n",
    "bucket_name = 'computer-vision-bootcamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2ce054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first step to define data processor\n",
    "processor_instance_count = 1\n",
    "processor_instance_type = 'local'#'ml.m5.large'\n",
    "\n",
    "processor = TensorFlowProcessor(framework_version = '2.3',\n",
    "                                role = aws_role,\n",
    "                                base_job_name = 'supermarket-image-augmantation',\n",
    "                                py_version = 'py37',\n",
    "                                instance_count = processor_instance_count,\n",
    "                                instance_type = processor_instance_type ,\n",
    "                                sagemaker_session = PipelineSession()\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfbc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.processing:Uploaded None to s3://sagemaker-us-east-1-527657206104/supermarket-image-augmantation-2023-05-02-08-18-45-627/source/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://sagemaker-us-east-1-527657206104/supermarket-image-augmantation-2023-05-02-08-18-45-627/source/runproc.sh\n",
      "INFO:sagemaker:Creating processing-job with name supermarket-image-augmantation-2023-05-02-08-18-45-627\n",
      "INFO:sagemaker.local.local_session:Starting processing job\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define processor step \n",
    "processor_args = processor.run(\n",
    "    code = 'preprocessing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name = 'manifest',\n",
    "            source = s3_manifest_file_path,\n",
    "            destination = '/opt/ml/processing/input/manifest'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name = 'images',\n",
    "            source = s3_images_path,\n",
    "            destination = '/opt/ml/processing/input/images'\n",
    "        )\n",
    "    ],\n",
    "    \n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='augmented_train_images',\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination= f'{bucket_name}/Processing-job-output/'\n",
    "            #,s3_upload_mode=\"Continuous\"\n",
    "        )\n",
    "\n",
    "    ],\n",
    "    arguments=[\n",
    "         \"--num_augmentations_per_img\", str(10),\n",
    "        \"--output_s3_bucket_name\", bucket_name        \n",
    "    ]\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_process_step = ProcessingStep(\n",
    "    name=\"supermarket-ImageAugmentation\",\n",
    "    step_args=data_processor_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418175b",
   "metadata": {},
   "source": [
    "## Prepare Training Job "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290897d",
   "metadata": {},
   "source": [
    "<font color='red'>**You can train using diffrent ways (use built-in algorithm , use pre-trained model , use your own code)**<font/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720af50",
   "metadata": {},
   "source": [
    "### start training job using pretrained model provided by aws "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63cc0a",
   "metadata": {},
   "source": [
    "## Fine-tune the pre-trained model on a custom dataset\n",
    "***\n",
    "\n",
    "Here we discuss how a model can be finetuned to a custom dataset with any number of classes.\n",
    "\n",
    "Transfer learning algorithm removes the object detection head of the pre-trained model and attaches a new randomly initialized head with number of classes same as the custom dataset. The fine-tuning step fine-tunes the last layer parameters while keeping the parameters of the rest of the model frozen, and returns the fine-tuned model. The objective during finetuning is to minimize box prediction error on the input data.\n",
    "\n",
    "- **Input** – A directory with sub-directory images and a file annotations.json.\n",
    "\n",
    "- **Output** – A fine-tuned model that can be deployed for inference or can be further trained using incremental training. A file mapping class indexes to class labels is saved along with the models.\n",
    "\n",
    "The input directory should look like below if the training data contains two images. The names of .png files can be anything.\n",
    "\n",
    "The s3 path should look like `s3://bucket_name/input_directory/`. Note the trailing `/` is required.\n",
    "\n",
    "    input_directory\n",
    "        |--images\n",
    "            |--abc.png\n",
    "            |--def.png\n",
    "        |--annotations.json\n",
    "\n",
    "The annotations.json file should have information for bounding_boxes and their class labels. It should have a dictionary with keys \"images\" and \"annotations\". Value for the \"images\" key should be a list of entries, one for each image of the form {\"file_name\": image_name, \"height\": height, \"width\": width, \"id\": image_id}. Value of the 'annotations' key should be a list of entries, one for each bounding box of the form {\"image_id\": image_id, \"bbox\": [xmin, ymin, xmax, ymax], \"category_id\": bbox_label}.\n",
    "\n",
    "We provide pennfudanped dataset as a default dataset for fine-tuning the model. PennFudanPed comprises images of pedestrians. The dataset has been downloaded from here.\n",
    "\n",
    "Citation: @ONLINE {pennfudanped, author = \"Liming Wang1, Jianbo Shi2, Gang Song2, and I-fan Shen1\", title = \"Penn-Fudan Database for Pedestrian Detection and Segmentation\", year = \"2007\", url = \"https://www.cis.upenn.edu/~jshi/ped_html/\" }\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4eb583",
   "metadata": {},
   "source": [
    "### Retrieve Training artifacts\n",
    "***\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained base model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the latest model. Also, we do need to specify the training_instance_type to fetch train_image_uri.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "model_id, model_version = \"tensorflow-od1-ssd-resnet101-v1-fpn-1024x1024-coco17-tpu-8\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6316d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "\n",
    "training_instance_type = 'local' #\"ml.p3.xlarge\"\n",
    "# you can get train_image_uri manually or using image_uris.retrieve()\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region='us-east-1',\n",
    "    framework = None,\n",
    "    model_id = model_id,\n",
    "    model_version= model_version,\n",
    "    image_scope='training',\n",
    "    instance_type=training_instance_type\n",
    ")\n",
    "\n",
    "# or you can manually selecet your uri image\n",
    "# model version can be found in\n",
    "# https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
    "# available images: https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "\n",
    "# train_image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.11.0-gpu-py39'\n",
    "\n",
    "\n",
    "# retrive training script \n",
    "training_script_uri = script_uris.retrieve(model_id= model_id, model_version=model_version,script_scope='training')\n",
    "\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve( model_id=model_id, model_version=model_version, model_scope='training')\n",
    "\n",
    "\n",
    "print(training_script_uri)\n",
    "# s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/tensorflow/transfer_learning/od1/v1.0.1/sourcedir.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da458346",
   "metadata": {},
   "source": [
    "### Set Training parameters\n",
    "***\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Object Detection model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. \n",
    "\n",
    "The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. \n",
    "\n",
    "The second set of parameters are algorithm specific training hyper-parameters.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0d9d9",
   "metadata": {},
   "source": [
    "***\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15032525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters \n",
    "\n",
    "# hyperparameters = hyperparameters.retrieve_default(model_id = model_id, model_version = model_version)\n",
    "\n",
    "\n",
    "# # # [Optional] Override default hyperparameters with custom values\n",
    "# hyperparameters[\"learning_rate\"] = \"0.01\"\n",
    "# hyperparameters[\"batch_size\"] = \"32\"\n",
    "# hyperparameters[\"optimizer\"] = \"adam\"\n",
    "# hyperparameters[\"train_only_top_layer\"] = \"True\"\n",
    "# hyperparameters[\"reinitialize_top_layer\"] = \"True\"\n",
    "# hyperparameters['epochs'] = 10\n",
    "# print(hyperparameters)\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"batch_size\": \"3\",\n",
    "    \"reinitialize_top_layer\": \"True\",\n",
    "    \"train_only_top_layer\": \"False\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"beta_1\": \"0.9\",\n",
    "    \"beta_2\": \"0.999\",\n",
    "    \"momentum\": \"0.9\",\n",
    "    \"epsilon\": \"1e-07\",\n",
    "    \"rho\": \"0.95\",\n",
    "    \"initial_accumulator_value\": \"0.1\",\n",
    "    \"early_stopping\": \"False\",\n",
    "    \"early_stopping_patience\": \"5\",\n",
    "    \"early_stopping_min_delta\": \"0.0\",\n",
    "    \"epochs\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8498522",
   "metadata": {},
   "source": [
    "### Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)) <a id='AMT'></a>\n",
    "***\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Define objective metric per framework, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"validation:localization_loss\", \"Regex\": \"Val_localization=([0-9\\\\.]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "training_metric_definitions = [\n",
    "    {\"Name\": \"val_localization_loss\", \"Regex\": \"Val_localization=([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"val_classification_loss\", \"Regex\": \"Val_classification=([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"loss=([0-9\\\\.]+).\"},\n",
    "]\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.001, 0.5, scaling_type=\"Logarithmic\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26bbb3",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job. This can take up to 30 minutes.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 1\n",
    "use_amt = True \n",
    "training_job_output_path = f's3://computer-vision-bootcamp/model_training/{model_version}'\n",
    "training_instance_count = 1\n",
    "training_instance_type = 'local' # 'ml.p3.xlarge' ##if you want to test locally choose local\n",
    "# Create Estimator to start trainting job \n",
    "\n",
    "estimator = Estimator(\n",
    "    role = aws_role,\n",
    "    image_uri = train_image_uri,\n",
    "    source_dir = training_script_uri,\n",
    "    model_uri = train_model_uri,\n",
    "    entry_point='transfer_learning.py', # this script is provided by training_script_uri\n",
    "    instance_count = training_instance_count,\n",
    "    instance_type = training_instance_type,\n",
    "    sagemaker_session = PipelineSession(),\n",
    "    enable_sagemaker_metrics=True,\n",
    "    hyperparameters=hyperparameters,\n",
    "    base_job_name = f'supermarket-ssd-{model_id}',\n",
    "    max_run = 360000,\n",
    "    volume_size=50,\n",
    "    output_path=f\"{training_job_output_path}\",\n",
    "    metric_definitions=training_metric_definitions\n",
    ")\n",
    "\n",
    "\n",
    "# Now we initialized our estimator we need to define our inputs as parameter\n",
    "training_data_input = TrainingInput(\n",
    "    s3_data=data_process_step.properties.ProcessingOutputConfig.Outputs[\"augmented_train_images\"].S3Output.S3Uri\n",
    ")\n",
    "validation_data_input = TrainingInput(\n",
    "    s3_data=data_process_step.properties.ProcessingOutputConfig.Outputs[\"augmented_validation_images\"].S3Output.S3Uri\n",
    ")\n",
    "\n",
    "print(training_data_input) ## to make sure it is correct i think i need to add /train/\n",
    "\n",
    "\n",
    "# if you chooce to hyperparameter tunning use_amt = True \n",
    "if use_amt:\n",
    "\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    training_args = hp_tuner.fit({\"training\": training_dataset_s3_path},)\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    training_args = estimator.fit({\"training\": training_dataset_s3_path}, logs=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "object_detection_model_training_step = TrainingStep(\n",
    "    name=\"super-market-training-job\",\n",
    "    step_args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9752db89",
   "metadata": {},
   "source": [
    "## Pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eeb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b102715",
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_object_detection_pipeline = Pipeline(\n",
    "    name=\"super-market-shelf-refill\",\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        training_instance_count\n",
    "    ],\n",
    "    steps=[\n",
    "        data_process_step,\n",
    "        object_detection_model_training_step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbfdb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_object_detection_pipeline.name,eager_object_detection_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67747c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_object_detection_pipeline.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(eager_object_detection_pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227d6f9",
   "metadata": {},
   "source": [
    "### Creating, updating and starting a pipeline\n",
    "\n",
    "Submit the pipeline definition to the Pipeline service. The Pipeline service uses the role that is passed in to create all the jobs defined in the steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_object_detection_pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = eager_object_detection_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f272a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd114a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aacac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33c72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f45c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af6202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
